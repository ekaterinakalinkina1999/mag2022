{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4029fb-f6d1-4764-a9c3-6916ec74a32a",
   "metadata": {},
   "source": [
    "### Теоретические основы КЛ в связи с программированием. Токенизация и сегментация по предложениям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71761bb7-bb76-46dd-a123-d547a3aa47ad",
   "metadata": {},
   "source": [
    "Современную лингвистику можно разделить на три больших области, которые тесно (более или менее) взаимодействуют друг с другом: это собственно теоретическая лингвистика, компьютерная лингвистика и NLP (natural language processing). В чем разница между этими тремя областями, чем они занимаются? Подробно можно посмотреть лекцию В.П. Селегея, которую он как-то читал у нас на открытом семинаре: [Компьютерная лингвистика сегодня: от исследований языка до киберспорта](https://youtu.be/sjDAdHPDtkc).\n",
    "\n",
    "Вкратце разница такова:\n",
    "- Теоретическая лингвистика занимается теоретическими (внезапно!) исследованиями. Теоретические лингвисты изучают язык в поле (в экспедициях) или с помощью корпусов. Даже теоретические лингвисты (современные) пользуются средствами автоматической обработки естественного языка! Питон многим из них облегчает жизнь: достаточно почитать последние научные статьи в журналах или осведомиться, какие диссертации защищались в последние годы. \n",
    "- NLP - это когда **инженеры** создают связанные с языком приложения преимущественно для бизнеса. Например, делают программу, которая позволяет автоматически обрабатывать кучи документов, или улучшают поисковые сервисы (или делают рекомендательные сервисы, или что угодно). \n",
    "- Компьютерная лингвистика - это когда **лингвисты** пользуются инструментами инженеров, такими, как нейронные сети, базы данных и т.д., чтобы изучать язык с теоретической точки зрения. \n",
    "\n",
    "Какой бы путь вы ни выбрали в дальнейшем, умение программировать пригодится... (тут должна была быть дежурная шутка про \"свободную кассу\", но увы, макдаки кончились). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd2d37-9c96-4f57-b5e5-5abe9db39120",
   "metadata": {},
   "source": [
    "В современном мире КЛ существует несколько базовых терминов, которые стоит знать всем. \n",
    "\n",
    "- Пайплайн: план действий, которые нужно выполнить, чтобы решить задачу. Например, если нам нужно собрать теоретический корпус языка, такой, как ГИКРЯ, нам нужно собрать и обработать тексты, разметить их и сделать так, чтобы по ним можно было искать. Пайплайн - это своего рода конвейер, когда вы в один конец трубы запихиваете свои данные, а на другом конце получаете что-то готовое. \n",
    "- Бейзлайн (есть вариант бейслайн): это базовое, самое очевидное решение какой-то задачи; самое первое, которое приходит в голову. Обычно от бейзлайна отталкиваются, когда хотят улучшить решение. Например, если у нас есть задача определять тональность текста (хороший/плохой отзыв), то самое простое - посчитать количество слов типа \"понравилось\\не понравилось\"; это будет бейзлайн, который будет с маленькой точностью эту задачу решать. Но на него уже можно равняться. \n",
    "- SOTA (State of the Art) - наилучшее решение какой-то задачи. Обычно в NLP все крутится вокруг того, чтобы побить SOTA-решение.\n",
    "- Датасет: набор данных для решения задачи. В нашем случае датасет обычно == корпус: для решения задач КЛ и NLP берутся наборы текстов, размеченных или нет. Размеченные датасеты на вес золота!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c1e83-6f2c-4ce4-9225-67a99e8f7622",
   "metadata": {},
   "source": [
    "Задачи NLP - это не просто какие-то там задачи, которые человек решил и забыл: это задачи очень сложные, про некоторые из них (как задача машинного перевода) вообще долгое время думали, что у них нет решения. Соответственно, этот набор задач живет и здравствует, а цель инженеров - улучшать существующие решения. Какие есть примеры таких \"вечных\" задач?\n",
    "- самая первая, пожалуй: машинный перевод. МП очень увлекались в 60-х, теория \"Смысл-Текст\" возникла именно из-за того, что в СССР много вкладывались в машинный перевод. К концу века люди серьезно разочаровались в МП, одно время верили, что невозможно эту задачу решить. Переворот случился, когда появились нейронные сети. Все современные переводчики - на них; вы можете сами заметить, что перевод того же гугла улучшается изо дня в день. \n",
    "- анализ тональности текста: дан миллион отзывов на продукт, требуется выяснить, сколько из них положительные, а сколько отрицательные, а в идеале еще и вычленить, к чему люди придираются и что хвалят. \n",
    "- извлечение именованных сущностей: в тексте встречаются имена собственные, даты, цены и т.п. Нужно автоматически их все вычленить. \n",
    "- извлечение отношений: а теперь не просто вычленить именованные сущности, но и понять, кто кому сват и брат (кто, что, за сколько и когда купил/продал). \n",
    "- генерация текста: Балабоба. Другие разновидности - генерация кода (!), генерация рецептов и миллион других вещей. Очень интересное!\n",
    "- саммаризация: есть \"Война и мир\", школьник хочет саммари. Машины умеют это!\n",
    "- упрощение текста: есть сложная научная статья, машина умеет ее переводить на человеческий язык. \n",
    "- автоматический бан троллей и провокаторов в интернете: вычленить из постов пользователя мат, забанить его: машина умеет это!\n",
    "\n",
    "...\n",
    "\n",
    "Задач, как мы видим, много. У них свои собственные пайплайны и методы, но примерно все задачи, где для решения нужен текст, требуют этот текст обрабатывать определенным образом. Тут уже на сцену выходим мы, лингвисты. Во-первых, наши коллеги из физтеха, которые занимаются именно NLP, ничего так не любят, как просить студентов РГГУ разметить им вот еще один маленький тут датасетик. Во-вторых, некоторые виды разметки так сложны, что без парочки лингвистов и не обойтись: например, синтаксическая или семантическая разметка. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01cc77-556c-40b8-8254-5460f72ac6cd",
   "metadata": {},
   "source": [
    "Таким образом, для сбора датасетов существует приблизительно один общий пайплайн (некоторые задачи требуют все перечисленные шаги, другие нет). Как собирать датасет? (в частности, корпус) В том числе для ваших собственных теоретических изысканий. \n",
    "\n",
    "1. Необходимо получить данные. В связи этим есть такие термины, как краулинг (crawling) и скрейпинг (scraping). Тексты мы сегодня получаем преимущественно из интернета и полностью автоматически. Самая известная программа для краулинга - Apache Nutch; на ваше счастье, мы не будем учиться ей пользоваться. Есть и программы попроще, типа twint для скрейпинга твиттера, и у многих сайтов, например, у Википедии, есть собственный API (интерфейс для взаимодействия между двумя программами, в отличие от UI - интерфейса между человеком и программой). Если вам когда-нибудь понадобится выкачать тексты, можно воспользоваться этим сакральным знанием: для многих популярных ресурсов типа телеграма люди давно понаписали очень простых в использовании библиотек, например, для Вики есть wikipedia-api (но у Вики, кстати, есть и готовые дампы, которые можно получить по адресу: [тут](https://dumps.wikimedia.org/). Дело за малым: распарсить xml-разметку...). \n",
    "2. В зависимости от вашей удачи и старания написавших библиотеки людей, вышеназванные краулеры/скрейперы выкачивают текст либо уже готовеньким для обработки (но и то его обычно приходится *нормализовать*, то есть, выкидывать ссылки, смайлики и тому подобное), либо as is: со всей html-обвязкой и прочим бойлерплейтом (boilerplate - стандартизованный шаблонный текст, например, тексты в табличках Википедии, которые перечисляют, что нужно указать на странице). Тексты обычно приходится потом чистить: самый известный инструмент для вытаскивания текста из html - beautifulsoup4. Иногда дополнительно приходится удалять дубли (копипасту) и даже вообще отфильтровывать спам (для этого создаются собственные инструменты, обычно на нейронных сетях). \n",
    "3. Вот наконец мы получили гигантский набор текстовых файликов, внутри которых содержатся сырые тексты! Что теперь с ними делать? Компьютер не понимает, где в этих текстах слова и предложения: для него это только набор символов. Значит, уже лингвистическая задача - разделить эти тексты на предложения и потом на **токены**. \n",
    "\n",
    "        Токен - это значимая для анализа последовательность символов. Не обязательно слово и знак пунктуации: например, современные нейронные сети внутри себя делят тексты на подслова, которые нам кажутся бессмысленными. Нейронка может разделить слово \"Сингапура\" как \"Сингапур - а\", а может как \"С - и - нга - пур - а\"... real story. \n",
    "        \n",
    "4. Дальше уже может понадобиться более сложный анализ текстов: морфологическая, синтаксическая, семантическая разметка. Ничего из этого сегодня обычно не делают вручную. Мы с вами посмотрим, как автоматически делать морфологический и синтаксический разбор. \n",
    "\n",
    "5. В качестве вишенки на торте можно устроить еще какую-нибудь разметку: выделить в тексте именованные сущности, референтные связи и так далее, но это уже в зависимости от задачи. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27b799-f94a-4742-a09c-d38bfabf2dcf",
   "metadata": {},
   "source": [
    "Поскольку все это - вещи, которыми люди занимаются уже довольно давно, существует просто *огромное* количество готовых инструментов, в том числе большие комплексные библиотеки; подавляющее большинство этих инструментов написано для питона! Поэтому мы его и изучаем. (Точнее говоря, написаны они обычно в каких-нибудь С или Java, но добрые люди понаделали \"оберток\" под питон). \n",
    "\n",
    "Большие библиотеки, о которых стоит знать:\n",
    "- NLTK: самая, пожалуй, старая и известная. \n",
    "- TextBlob: тот же NLTK, только в профиль. \n",
    "- spaCy: более современный вариант NTLK на Cython (что это, можно не знать. :)\n",
    "- stanza (Stanford NLP) - чуть похуже спейси, но тоже мультиязычная библиотека.\n",
    "- DeepPavlov: скорее заточенная под нужды инженеров библиотека, которую разрабатывают в МФТИ. Для русского языка! (все вышеназванные мультиязычные)\n",
    "- natasha: тоже сделана для русского языка, очень быстрая, достаточно качественная. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138b55b-4dcc-4a5d-94c9-1d75fe849859",
   "metadata": {},
   "source": [
    "Последнее, о чем стоит знать - это принципы, на которых базируются наши инструменты. На чем может работать инструмент автоматической обработки естественного языка:\n",
    "1. На правилах: человек сам прописывает лес ифов. Плюс правиловых инструментов: вы всегда точно знаете, как он будет работать. А еще он будет быстрый, как понос. Минус: невозможно предусмотреть вообще ВСЕ, трудно дорабатывать, качество работы обычно самое низкое. \n",
    "2. С использованием статистических (вероятностных) методов: мы все еще понимаем, как и почему работает наш инструмент, но уже вместо правилок используем теорию вероятности. Ну типа, с какой вероятностью после предлога будет идти глагол?..\n",
    "3. С использованием алгоритмов машинного обучения: уже не очень хорошо понимаем, что происходит внутри у нашего инструмента (а кстати, он стал медленнее...), но точно знаем, какие данные нужно ему скормить, чтобы он работал лучше. Машинное обучение - это когда мы показываем компьютеру, как надо правильно делать, а он пытается сам по хитрым математическим формулам вычислить ответы на основании того, что видел. \n",
    "4. На нейронных сетях: абсолютно не понимаем, что происходит у нашего инструмента внутри, и работает он, как черепаха. Зато ему достаточно скормить примерчики, как правильно решать задачу, и он будет удивительно качественно ее решать! Чем больше данных для обучения, тем качественнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bbf4e5-1a62-48ce-8e54-df9ba809bd69",
   "metadata": {},
   "source": [
    "Большинство современных задач (подавляющее) уже решается только нейронными сетями: язык - вещь непростая, и морфологию или синтаксис правилами сегодня размечают только безумцы. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddacd0b-b469-4581-a571-9b7e1397da35",
   "metadata": {},
   "source": [
    "#### Токенизация и сегментация по предложениям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b6c4e-1063-4e77-b48a-78f293c66f50",
   "metadata": {},
   "source": [
    "И то, и другое - очень базовые задачи; токенизаторы и сегментаторы включены во все крупные лингвистические библиотеки. \n",
    "\n",
    "Поскольку разделять текст на слова и предложения вроде бы очень легко, обычно используются инструменты на правилах; хотя и не для каждого языка это верно: для языков, у которых слова не отделяются пробелами, токенизаторы могут быть нейронными. Кстати, неплохо вспомнить, что мы сами не всегда знаем, что мы хотим считать за слово. Что мы хотим считать за токен - другой вопрос: токен определяется задачами. Например, для определенной задачи мы можем хотеть считать, что \"бледно-зеленый\" - это два отдельных слова, а для другой - нет. \n",
    "\n",
    "Самый простой токенизатор вы в состоянии написать сами. Иногда это бывает даже хорошо и полезно, потому что про свой токенизатор вы точно знаете, как он работает. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3146d0d4-2565-4e32-90fb-790395a7d7d9",
   "metadata": {},
   "source": [
    "Итак, токенизаторы и сегментаторы из коробки. И то, и другое необходимо устанавливать. \n",
    "\n",
    "1. Сентенайзеры:\n",
    "\n",
    "        pip install razdel\n",
    "        pip install rusenttokenize\n",
    "        pip install nltk\n",
    "        # и потом в интерактивном режиме выполнить команды:\n",
    "        import nltk\n",
    "        nltk.download()\n",
    "        # и загрузить как минимум популярные пакеты\n",
    "        \n",
    "В Colab тоже нужно устанавливать их: это можно сделать, если в ячейке написать:\n",
    "\n",
    "        !pip install (и так далее)\n",
    "        \n",
    "И запустить ячейку. К сожалению, поскольку колаб - облачная среда, в отличие от локального компьютера, придется устанавливать все модули заново при каждом новом запуске тетрадки. \n",
    "\n",
    "razdel - подмодуль Наташи; rusenttokenize - подмодуль DeepPavlov. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d40493-fde6-4d76-a75a-9660799faebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Тут длиннющий текст с много предложений.', 'Чтобы питон считал много строчек как одну, нужно брать их в тройные кавычки.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "raw = '''Тут длиннющий текст с много предложений. Чтобы питон считал много строчек как одну, нужно брать их в тройные кавычки.'''\n",
    "\n",
    "print(sent_tokenize(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd75f85b-6668-47ec-857c-7eb2d4db384a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Тут длиннющий текст с много предложений.',\n",
       " 'Чтобы питон считал много строчек как одну, нужно брать их в тройные кавычки.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import sentenize\n",
    "\n",
    "raw = '''Тут длиннющий текст с много предложений. Чтобы питон считал много строчек как одну, нужно брать их в тройные кавычки.'''\n",
    "\n",
    "sents = [s.text for s in sentenize(raw)]\n",
    "\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b841d-dc3c-447f-afc8-4768c4d7a08e",
   "metadata": {},
   "source": [
    "Для чего нам тут нужен генератор? Дело в том, что sentenize работает примерно как finditer: возвращает итератор из набора специальных объектов, очень похожих на Match из re, в которых содержится сам текст предложения + индексы его начала и конца в исходной строке. Обычно эти индексы никому не нужны, поэтому результат тут же пересобирывается в список строк. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa1d62a-f15f-463b-bea2-b4ac61f5f3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Тут длиннющий текст с много предложений.',\n",
       " 'Чтобы питон считал много строчек как одну, нужно брать их в тройные кавычки.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rusenttokenize import ru_sent_tokenize\n",
    "\n",
    "raw = '''Тут длиннющий текст с много предложений. Чтобы питон считал много строчек как одну, нужно брать их в тройные кавычки.'''\n",
    "\n",
    "sents = ru_sent_tokenize(raw)\n",
    "\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac20f3-5548-4f99-b4fc-f9bba3e8ea9d",
   "metadata": {},
   "source": [
    "На таком простом примере оба сентенайзера работают одинаково, но советую поэкспериментировать с более сложными текстами. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b39d1c-2786-48d0-baad-0575e45c868b",
   "metadata": {},
   "source": [
    "Токенизаторы устроены примерно так же. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4e1949-296a-42f6-a690-f30d5295f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Тут', 'длиннющий', 'текст', 'с', 'много', 'предложений', '.', 'Чтобы', 'питон', 'считал', 'много', 'строчек', 'как', 'одну', ',', 'нужно', 'брать', 'их', 'в', 'тройные', 'кавычки', '.']\n",
      "['Тут', 'длиннющий', 'текст', 'с', 'много', 'предложений', '.', 'Чтобы', 'питон', 'считал', 'много', 'строчек', 'как', 'одну', ',', 'нужно', 'брать', 'их', 'в', 'тройные', 'кавычки', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize # есть еще RegexpTokenizer(), куда можно регуляркой задать, какие штуки мы хотим считать за токены\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "print(tokenizer.tokenize(raw))\n",
    "print(word_tokenize(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a1fcb3-f900-450e-a7d4-0143a0681d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Тут',\n",
       " 'длиннющий',\n",
       " 'текст',\n",
       " 'с',\n",
       " 'много',\n",
       " 'предложений',\n",
       " '.',\n",
       " 'Чтобы',\n",
       " 'питон',\n",
       " 'считал',\n",
       " 'много',\n",
       " 'строчек',\n",
       " 'как',\n",
       " 'одну',\n",
       " ',',\n",
       " 'нужно',\n",
       " 'брать',\n",
       " 'их',\n",
       " 'в',\n",
       " 'тройные',\n",
       " 'кавычки',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "tokens = [t.text for t in tokenize(raw)]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd69d36-fdbb-4628-bed5-df50fc10ce68",
   "metadata": {},
   "source": [
    "Для третьего токенизатора нужно установить библиотеку SpaCy. Подробнее ее скоро обсудим. Она устанавливается в командной строке двумя строчками:\n",
    "\n",
    "    pip install spacy\n",
    "    python -m spacy download ru_core_news_md\n",
    "    \n",
    "(Вторая строчка - для загрузки модели, они бывают разные, это тоже посмотрим). \n",
    "\n",
    "Спейси - библиотека-комбайн, которая автоматически сразу за вас делает все, поэтому у нее немного своеобразный синтаксис. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3247216b-17ce-470d-8fea-e537d9f05d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Тут',\n",
       " 'длиннющий',\n",
       " 'текст',\n",
       " 'с',\n",
       " 'много',\n",
       " 'предложений',\n",
       " '.',\n",
       " 'Чтобы',\n",
       " 'питон',\n",
       " 'считал',\n",
       " 'много',\n",
       " 'строчек',\n",
       " 'как',\n",
       " 'одну',\n",
       " ',',\n",
       " 'нужно',\n",
       " 'брать',\n",
       " 'их',\n",
       " 'в',\n",
       " 'тройные',\n",
       " 'кавычки',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('ru_core_news_md') # загрузим нашу модель для русского\n",
    "doc = nlp(raw) # на самом деле в этот момент спейси предобрабатывает наш сырой текст \n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f608dbf-7bcc-4011-aa97-986ba5bdbe9e",
   "metadata": {},
   "source": [
    "#### Морфопарсинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab587305-020c-4d74-a3ca-ca5adc96cca3",
   "metadata": {},
   "source": [
    "Установка нужных для него библиотек:\n",
    "\n",
    "**pymorphy2**\n",
    "\n",
    "*pip install pymorphy2*\n",
    "\n",
    "*pip install -U pymorphy2-dicts-ru*  # необязательно: для обновления словаря\n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**Mystem**\n",
    "\n",
    "*pip install git+https://github.com/nlpub/pymystem3* \n",
    "\n",
    "(возможно, уже перестало работать для новых версий питона: можно установить просто как pip install pymystem3, но тогда лучше не запускать на нем длинные тексты с переносами на новую строку.)\n",
    "\n",
    "*Colab*: гарантированно не работает. \n",
    "\n",
    "**rnnmorph**\n",
    "\n",
    "*pip install rnnmorph*\n",
    "\n",
    "Может козлить во время установки. Иногда, если плохо установился и не работает, приходится его удалять командой pip uninstall rnnmorph (обязательно в консоли от имени администратора!). Периодически, если обновляется версия tensorflow, может перестать работать &ndash; но Илья Гусев обычно вскоре обновляет и свой парсер, так что достаточно следить за новостями в [его гитхабе](https://github.com/IlyaGusev/rnnmorph) &ndash; или просто подождать светлых времен.\n",
    "\n",
    "*Colab*: обычно работает без проблем.\n",
    "\n",
    "**pyconll**\n",
    "\n",
    "*pip install pyconll*\n",
    "\n",
    "*Colab*: устанавливается без проблем. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5606e-9f55-41a6-a744-0b3da703132a",
   "metadata": {},
   "source": [
    "Почти все нижеописанные инструменты в питоне устроены довольно однообразно: имеется основной класс \"парсер\", экземпляр которого вам нужно создать, прежде чем что-то парсить. То есть, условно говоря, из набора машинок берете ту конкретную, которая вас повезет. Когда создан экземпляр класса, ему уже можно скармливать свои тексты. \n",
    "\n",
    "Стемминг &ndash; это уже чисто историческое, можно сказать, явление: в 1980-х, когда еще не было даже графического интерфейса у компьютеров и тем более средств автоматического морфоразбора, Мартин Портер разработал свой алгоритм стемминга: усечение окончания от псевдоосновы. Этот алгоритм так и называется \"стеммер Портера\" и доступен в версиях для нескольких европейских языков, в т.ч. для русского (Snowball &ndash; чуть более новая версия). Алгоритм с помощью правил отсекает окончания и суффиксы, основываясь на особенностях языка. Как все правиловое, работает не без ошибок. \n",
    "\n",
    "Код просто посмотреть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1591262c-230d-4aa0-a1a8-e8fa0853f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пердикк\n",
      "не\n",
      "мен\n",
      "десят\n",
      "раз\n",
      "заключа\n",
      "и\n",
      "расторга\n",
      "союз\n",
      "с\n",
      "основн\n",
      "участник\n",
      "войн\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')  # экземпляр класса \n",
    "example = ['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n",
    "for token in example:\n",
    "    print(stemmer.stem(token))  # stem() - метод класса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb844d-5570-4f3a-81a3-f3816feba662",
   "metadata": {},
   "source": [
    "Самые простые &ndash; правиловые морфопарсеры. Для русского языка их два: pymorphy2 и pymystem3. Pymorphy был создан Михаилом Коробовым (вот его известная [статья на хабре](https://habr.com/ru/post/176575/)) как аналог Майстем. Он работает на словаре и использует тагсет [OpenCorpora](http://opencorpora.org/)), а также статистику, предпосчитанную на этом корпусе. \n",
    "Как устроен pymorphy2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f710ca1-eca3-440d-8504-6628be43e7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn sing,gent'), normal_form='студентка', score=0.6, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 1),)),\n",
       " Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn plur,nomn'), normal_form='студентка', score=0.4, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 7),))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "parse = morph.parse('студентки')\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54c2ea-ec56-448f-910c-0fe1b3d5afcf",
   "metadata": {},
   "source": [
    "У класса MorphAnalyzer() есть метод parse, который возвращает что? Список экземпляров класса Parse. У этого класса есть свои атрибуты: word (исходная форма слова), tag (грам. инфа), normal_form (лемма), score(предпосчитанная на OpenCorpora вероятность правильности разбора) и несколько технических. \n",
    "\n",
    "Соответственно, получить информацию можно, просто обращаясь к атрибутам (не забудьте, что у нас всегда список, поэтому нужно еще и индекс разбора указывать):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd6f146-0858-4e59-abb0-a38000c16128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "студентки\n",
      "NOUN,anim,femn sing,gent\n",
      "студентка\n"
     ]
    }
   ],
   "source": [
    "print(parse[0].word)\n",
    "print(parse[0].tag)\n",
    "print(parse[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663a55c-ff9b-46ab-b9ad-be2466338895",
   "metadata": {},
   "source": [
    "Атрибут tag &ndash; это экземпляр класса OpencorporaTag, как можно догадаться. У него есть еще свои атрибуты, к которым тоже можно обращаться, чтобы получать более конкретную информацию о слове. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d4f6ca-ce57-4c0f-8c35-33249e5787a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: NOUN\n",
      "Одушевленность: anim\n",
      "Падеж: nomn\n",
      "Род: masc\n",
      "Наклонение: None\n",
      "Число: sing\n",
      "Лицо: None\n",
      "Время: None\n",
      "Переходность: None\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('участник')\n",
    "\n",
    "t = parse[0].tag  # я записала в переменную, просто чтобы не копировать каждый раз все целиком\n",
    "# но это то же самое, что parse[0].tag.animacy...\n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\nНаклонение: {t.mood}\\\n",
    "\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa50d3db-1ee9-4073-b99b-e8e5e47413ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: VERB\n",
      "Одушевленность: None\n",
      "Падеж: None\n",
      "Род: None\n",
      "Наклонение: indc\n",
      "Число: sing\n",
      "Лицо: 3per\n",
      "Время: pres\n",
      "Переходность: tran\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('говорит')\n",
    "\n",
    "t = parse[0].tag  \n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\n\\\n",
    "Наклонение: {t.mood}\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab7b87-6f0e-43cc-9d44-a2e31bfeedda",
   "metadata": {},
   "source": [
    "Если вы запрашиваете категорию, которой у данного слова нет (ну нет переходности у существительного), вернется None. \n",
    "\n",
    "Также можно попросить pymorphy поставить слово в конкретную форму или вообще вернуть всю парадигму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cee63ad0-5f72-4909-9c77-ee7df8292599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='говорят', tag=OpencorporaTag('VERB,impf,tran plur,3per,pres,indc'), normal_form='говорить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'говорят', 415, 6),))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].inflect({'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77598b15-1e05-4497-a4ba-4be057e3d127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='участник', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участник', 2, 0),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 1),)),\n",
       " Parse(word='участнику', tag=OpencorporaTag('NOUN,anim,masc sing,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнику', 2, 2),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 3),)),\n",
       " Parse(word='участником', tag=OpencorporaTag('NOUN,anim,masc sing,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участником', 2, 4),)),\n",
       " Parse(word='участнике', tag=OpencorporaTag('NOUN,anim,masc sing,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнике', 2, 5),)),\n",
       " Parse(word='участники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участники', 2, 6),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 7),)),\n",
       " Parse(word='участникам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участникам', 2, 8),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 9),)),\n",
       " Parse(word='участниками', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниками', 2, 10),)),\n",
       " Parse(word='участниках', tag=OpencorporaTag('NOUN,anim,masc plur,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниках', 2, 11),))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].lexeme  \n",
    "# парадигму глагола лучше не выводить - она длинная; я перед запуском этой ячейки перезапустила разбор с существительным, поэтому не удивляйтесь. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8877d22-7da2-4aa3-a85a-218b2e2bd0b6",
   "metadata": {},
   "source": [
    "Наконец, можно попросить pymorphy выводить грам. информацию по-русски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71534475-0908-4dbc-be7f-b569d47d1ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СУЩ,од,мр ед,им'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ce5dd-fd72-4491-b0bf-fbb19ac4b9b5",
   "metadata": {},
   "source": [
    "Pymorphy очень быстро работает и имеет много возможностей, но совершенно не умеет разрешать омонимию и никак не учитывает контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312a4cd-a0dd-47b3-8964-d7961d83359b",
   "metadata": {},
   "source": [
    "Алгоритм, легший в основу Mystem, разрабатывался в ИППИ и был первым вообще для русского языка; его в свое время купил у них Илья Сегалович, доработал, опубликовал собственную статью. Поисковик Яндекса когда-то работал на майстеме. Сам парсер написан в С (для скорости: бинарный поиск в питоне реализовать можно только с внешними библиотеками на С, а у майстема 2 словаря, по которым нужно искать). Для питона под него сделана оболочка (pymystem3). Майстем капризный, тяжело запускается, имеет не так много функций, но работает тоже довольно быстро и умеет доносить на бастардов: сообщать, что слово не найдено в его словаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07120ed6-7a8b-4d2e-a78b-1192b4784dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "\n",
    "m = pymystem3.Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4fc97-c226-4e84-9b09-de107e8e084a",
   "metadata": {},
   "source": [
    "Майстем принимает только сырой текст в виде одной строки: у него встроенный токенизатор, потому что он пытается учитывать контекст. Поэтому стоит указывать entire_input=False при создании экземпляра класса, чтобы он не выводил вообще все, включая пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89bb4d5-e98e-42b6-86de-86b6e9bd2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = '''Пердикка II (др.-греч. Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э. После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка. На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере. На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах. С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом. Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "206e5665-3bc6-4102-8ba3-1470992a76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = m.lemmatize(raw)\n",
    "analysis = m.analyze(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbeed29d-79e4-41da-92d2-74a1b26e9053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пердикк',\n",
       " 'II',\n",
       " 'др',\n",
       " 'греча',\n",
       " 'Περδίκκας',\n",
       " 'Β',\n",
       " 'της',\n",
       " 'Μακεδονίας',\n",
       " 'македонский',\n",
       " 'царь']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[:10]  # надеюсь, греча вас тоже порадовала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d273193-54b5-4ca7-b4ec-17503cf4ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'пердикк',\n",
       "    'qual': 'bastard',\n",
       "    'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Пердикка'},\n",
       " {'analysis': [], 'text': 'II'},\n",
       " {'analysis': [{'lex': 'др', 'gr': 'S,сокр,мн,неод=(пр|вин|дат|род|твор|им)'}],\n",
       "  'text': 'др'},\n",
       " {'analysis': [{'lex': 'греча', 'gr': 'S,жен,неод=род,мн'}], 'text': 'греч'},\n",
       " {'analysis': [], 'text': 'Περδίκκας'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcac691-5ee8-4d90-8a12-b4a1dd0cddf1",
   "metadata": {},
   "source": [
    "С леммами вроде все должно быть понятно, а что зашито в анализе?\n",
    "\n",
    "Майстем возвращает список. Каждый токен в этом списке - это словарь с ключами analysis & text. Первого ключа может не быть: если у нас знак пунктуации. Если же он есть, то в нем содержится список (обычно состоящий из одного элемента - если не указать при создании экземпляра класса Mystem glue_grammar_info=False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d3de67e-5de5-4421-80d7-77ba72f49ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово: {'analysis': [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}], 'text': 'Пердикка'}\n",
      "Его грам. инфа: [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}]\n",
      "Его оригинальная форма: Пердикка\n",
      "Какие есть ключи в словаре с разбором: dict_keys(['lex', 'qual', 'gr'])\n",
      "Лемма: пердикк\n",
      "Этот ключ бывает только тогда, когда слова нет в словаре: bastard\n",
      "А это грам. информация: S,имя,муж,од=(вин,ед|род,ед)\n"
     ]
    }
   ],
   "source": [
    "print(f'Первое слово: {analysis[0]}')\n",
    "print(f\"Его грам. инфа: {analysis[0]['analysis']}\\nЕго оригинальная форма: {analysis[0]['text']}\")\n",
    "print(f\"Какие есть ключи в словаре с разбором: {analysis[0]['analysis'][0].keys()}\")\n",
    "print(f\"Лемма: {analysis[0]['analysis'][0]['lex']}\\n\\\n",
    "Этот ключ бывает только тогда, когда слова нет в словаре: {analysis[0]['analysis'][0]['qual']}\\n\\\n",
    "А это грам. информация: {analysis[0]['analysis'][0]['gr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e2755-8fa1-4742-87ba-38da385aeb8a",
   "metadata": {},
   "source": [
    "Непросто, да. Еще сложнее устроен ключ 'gr', который содержит грамматическую информацию о слове: обычно майстем склеивает варианты разбора, то есть, выше запись следует читать как \"существительное, имя собственное, мужского рода, одушевленное; возможно, Acc Sg, а возможно, Gen Sg. \n",
    "\n",
    "Вот как раз если указать, чтобы грам. информация не склеивалась, майстем будет возвращать несколько словарей с вариантами по отдельности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3cbd255-1cf1-4d4c-b747-894fcca149f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis': [{'lex': 'пердикк',\n",
       "   'qual': 'bastard',\n",
       "   'gr': 'S,имя,муж,од=вин,ед'},\n",
       "  {'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=род,ед'}],\n",
       " 'text': 'Пердикка'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_noglue = pymystem3.Mystem(entire_input=False, glue_grammar_info=False)\n",
    "\n",
    "noglueanalysis = m_noglue.analyze(raw)\n",
    "noglueanalysis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a0db9-3ef9-429f-b338-2b5606b1dee0",
   "metadata": {},
   "source": [
    "Теперь о вещах, которых нет в стабильной версии Mystem, а есть только в той, которая устанавливается через git:\n",
    "\n",
    "1. Майстем очень плохо умеет обрабатывать \\n. Когда он получает строку, в которой много \\n (а это неизбежно, ведь мы чаще хотим обрабатывать длиннющие тексты), на каждом \\n он перезапускает свой бинарник (написанный в С). Поэтому на длинных текстах работать будет ОЧЕНЬ медленно (впрочем, все равно быстрее нейронок...). Чтобы решить эту проблему - ведь замена \\n на пробелы, например, искажает контекст - сделали возможность особым образом обрабатывать \\n, когда загружаем текст из файла. \n",
    "2. Есть функция, которая позволяет получить часть речи для конкретного токена. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed5be5-c98a-42fa-a45c-654429a4174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = m.analyze(file_path=path) # можно напрямую передавать в майстем путь к файлу с текстом - он сам откроет и обработает как ему надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90f45179-5d11-4c6a-97c0-f0fbe491c05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_pos(analysis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f42f7a-9bac-42a3-9222-e074d67a41b2",
   "metadata": {},
   "source": [
    "В 2017 году на соревновании конференции \"Диалог\" победила команда Гусев-Анастасьев: ребята создали морфопарсер для русского языка на нейронной сети. Он называется rnnmorph (RNN - это название модели нейронной сети, на которой он работает). Гусев при создании явно вдохновлялся pymorphy2 (кстати, Коробов для этого же соревнования сделал библиотеку для приведения разных тагсетов к одному): синтаксис rnnmorph очень похож на pymorphy2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64772bd0-83a0-4b02-8df2-746ca6f510c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 12:00:12.010260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 12:00:12.010289: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-06 12:00:13.910063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-06 12:00:13.910109: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-06 12:00:13.910134: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aslin): /proc/driver/nvidia/version does not exist\n",
      "2022-04-06 12:00:13.910346: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<normal_form=пердикк; word=Пердикка; pos=NOUN; tag=Case=Acc|Gender=Masc|Number=Sing; score=0.7922>,\n",
       " <normal_form=не; word=не; pos=PART; tag=_; score=1.0000>,\n",
       " <normal_form=менее; word=менее; pos=ADV; tag=Degree=Cmp; score=1.0000>,\n",
       " <normal_form=десять; word=десяти; pos=NUM; tag=Case=Gen; score=1.0000>,\n",
       " <normal_form=раз; word=раз; pos=NOUN; tag=Case=Gen|Gender=Masc|Number=Plur; score=0.9998>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "\n",
    "predictor = RNNMorphPredictor(language='ru')\n",
    "parse = predictor.predict(['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.'])\n",
    "parse[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01366108-8fe1-4cb1-9cbd-bac3015b2e55",
   "metadata": {},
   "source": [
    "rnnmorph принимает список строк (токенов) и возвращает список объектов специального класса, у которого есть атрибуты normal_form (лемма), word (исходная форма), pos (часть речи), tag (грам. информация) и score (уверенность нейронной модели в правильности своего ответа). Он умеет снимать омонимию (лучше, чем майстем, но хуже, чем интегральный морфопарсер). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7704537-8f36-4588-b591-f8b786fbcc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[-2].pos  # например, можно узнать часть речи для предпоследнего слова в списке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cb9be-6175-44eb-a854-6036ca67e295",
   "metadata": {},
   "source": [
    "*Примечание*\n",
    "\n",
    "RNNMorph, как и любая нейронная модель, умеет работать на GPU (видеокарте). \n",
    "\n",
    "Для любопытных: нейронные сети &ndash; это, по сути, бесконечное умножение гигантских матриц друг на друга. Нейронная сеть состоит из кучи нейронов-функций, и когда она должна выдать ответ, она получает входные данные в виде таких же матриц на первый слой, который состоит из миллиона нейронов, этот миллион нейрончиков кидается высчитывать результат зашитой внутри них функции (что-то похожее на ax + b), а их ответы получает второй слой таких же нейрончиков, и так пока не получится финальный ответ. То есть, мы производим миллиарды однотипных вычислений. Процессор видеокарты устроен как раз таким образом, что он супер-быстро умеет считать однотипные вещи (он считает их батчами &ndash; сразу пачками), то есть, он работает гораздо быстрее, чем CPU, но обучен именно однотипно считать. Поэтому нейронки обычно и работают на GPU, они просто созданы друг для друга!\n",
    "\n",
    "Когда нейронка (и RNNMorph тоже) работает на видеокарте, она делает это обычно быстрее, чем на CPU. Но чтобы заставить RNNMorph перейти на видеокарту, нужно сложно настраивать ее (и иметь совместимую видеокарту до кучи); поэтому RNNMorph умеет, конечно, работать и на обычном процессоре, но при этом вываливает предупреждения про то, что настройки видеокарты он не нашел (они выше выделены красным). Их можно смело игнорировать! Это предупреждения, а не ошибки. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd926f9e-ff98-40cc-abf5-19e0dcf3911b",
   "metadata": {},
   "source": [
    "Последнее, о чем мы поговорили - это формат разметки Universal Dependencies и UDPipe. Из питона запускать UDPipe поучимся в следующий раз, но у него есть версия онлайн на [сайте](https://lindat.mff.cuni.cz/services/udpipe/). \n",
    "\n",
    "[Universal Dependencies](https://universaldependencies.org/) - это грандиозный существующий с 2006 года проект (сперва чешских, а потом и самых разных лингвистов, у нас в России им активно занимается О. Ляшевская), который ставит целью разработать такой формат морфосинтаксической разметки, который был бы одинаково применим к самым разным языкам. То есть, основная его фича - это *единообразие*, из-за чего, к примеру, принято решение в русском языке частицу \"не\" считать advmod (так она себя ведет в германских языках...), а копулу не считать вершиной (потому что копула в агглютинативных языках обычно отсутствует, ср. русское \"петя был учителем\" vs турецкое \"Petya öğretmendi\", где -di - показатель прошедшего времени, присоединяющийся к *существительному*). \n",
    "\n",
    "UD для разметки использует формат файлов .conllu, которые представляют собой таблички (мы с таким на прошлых семинарах имели дело уже). В этом формате существует 10 колонок, каждая ячейка в строке отделяется знаком табуляции; предложения разделяются пустой строкой. На самом сайте UD очень много полезной информации, в том числе описание этого формата и сборник ссылок на приложения, которыми его удобно читать!\n",
    "\n",
    "В питоне мы пытались написать некую читалку формата сами, но на самом деле, конечно, уже существуют готовые библиотеки для этого (и не одна, но мы посмотрим одну). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad19ad-6f92-4874-8dbb-3594ae976a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "text = pyconll.load_from_file('myfile.conllu')\n",
    "\n",
    "for sentence in text:\n",
    "    for token in sentence:\n",
    "        print(token.id, token.form, token.lemma, token.upos, token.feats, token.head, token.deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe8361-a0d1-4a47-a5fd-717aaf23d3f6",
   "metadata": {},
   "source": [
    "Естественно, можно не только печатать информацию, но и добавлять в список и вообще делать все, что угодно. Что это за атрибуты у токенов?\n",
    "\n",
    "- id - порядковый номер токена в предложении\n",
    "- form - исходная форма\n",
    "- lemma - лемма\n",
    "- upos - часть речи в UD\n",
    "- xpos - часть речи в неуниверсальном формате (обычно встречается, если датасет конвертированный)\n",
    "- feats - грам. характеристики\n",
    "- head - расстояние от синтаксической вершины\n",
    "- deprel - тип синтаксической связи\n",
    "- две зарезервированные ячейки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b11a3-9199-4d4c-9245-70abf0d2fb5d",
   "metadata": {},
   "source": [
    "Где можно красивенько отрисовать .conllu файлы в виде деревьев зависимости:\n",
    "\n",
    "[Арборатор](https://arborator.ilpga.fr/q.cgi): достаточно вставить текст в формате .conllu\n",
    "\n",
    "[Conllu-Viewer на сайте UD](https://universaldependencies.org/conllu_viewer.html): умеет читать файлы и рисовать последовательно все предложения\n",
    "\n",
    "Для затравки вот картинка с арборатора:\n",
    "\n",
    "<img src='arbo.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
