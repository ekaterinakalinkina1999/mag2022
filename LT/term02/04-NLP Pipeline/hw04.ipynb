{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6641c010-48de-4039-ba50-716afb34e33e",
   "metadata": {},
   "source": [
    "#### Задача 1. \n",
    "\n",
    "Посчитайте, как часто в выбранном вами тексте прилагательное стоит сразу перед существительным, а как часто - наоборот. Достаточно вывести количество вхождений (столько-то раз встретилось прил + сущ, столько-то раз - сущ + прил). Подумайте, насколько можно доверять этим цифрам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c356f-723d-4bbc-8b4f-f3d247754c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize
from nltk.util import ngrams
import collections

punctuations = ".?!,:;’-–—”()[]{}<>“/…*&#~\@^|"
txt_no_punc = ""

with open('The_Old_Man_and_the_Sea.txt', 'r', encoding='utf-8') as file:
    text = file.read().lower().replace('\n', ' ')
    for char in text:
        if char not in punctuations:
            txt_no_punc += char

    txt = pos_tag(word_tokenize(txt_no_punc))

    all_n_grams = list(ngrams(txt, 2))

    pairsNNJJ = []
    for elem in all_n_grams:
        ((w1, t1), (w2, t2)) = elem
        if t1 == "NN" and t2 == "JJ":
            pairsNNJJ.append((t1, t2))

    pairsJJNN = []
    for elem in all_n_grams:
        ((w1, t1), (w2, t2)) = elem
        if t1 == "JJ" and t2 == "NN":
            pairsJJNN.append((t1, t2))


    frequency_NNJJ = collections.Counter(pairsNNJJ)
    dct_frequency_NNJJ = dict(frequency_NNJJ)

    frequency_JJNN = collections.Counter(pairsJJNN)
    dct_frequency_JJNN = dict(frequency_JJNN)


    print("встретилось прил+сущ: ", dct_frequency_JJNN, "встретилось сущ+прил: ", dct_frequency_NNJJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86c048-770f-4f1b-b803-fb6e6d717a9e",
   "metadata": {},
   "source": [
    "#### Задача 2 (3 балла). \n",
    "\n",
    "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
    "\n",
    "- составить список таких предлогов (РГ-80 вам в помощь)\n",
    "- взять достаточно большой текст (можно большое художественное произведение)\n",
    "- сделать морфоразбор этого текста (лучше не pymorphy)\n",
    "- Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n",
    "\n",
    "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести). \n",
    "\n",
    "Если будете использовать RNNMoprph, возможно, понадобится регулярное выражение и немного терпения. \n",
    "\n",
    "Some prepositions in Russian may govern several cases, e.g. *in* can govern Accusative (*Я еду в Лондон*) or Locative (*Я живу в Лондоне*). Let's analyse these prepositions. You should do:\n",
    "\n",
    "- create a list of such prepositions (you may use [this](http://rusgram.narod.ru/1749-1772.html) - ask questions if you can't translate something)\n",
    "- take a large text (a big novel should do)\n",
    "- parse this text morphologically\n",
    "- count the cases of the words which follow our prepositions\n",
    "\n",
    "Marks: in fact any case may occur after a preposition due to free word order of Russian; a preposition may even occur at the end of the sentence (rarely). To take this into account you may count only pre-defined cases and not every case which occurs after the preposition. If you decide to use RNNMorph, you'll have to write a small regular expression in order to get the case from its tag (it's simple, actually). And beware that RNNMorph is *slow*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45434802-656e-4cc0-b0d3-f32dcde9320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy
from nltk.util import ngrams
import collections

with open('Prestuplenie_i_nakazanie.txt', 'r', encoding='utf-8') as file:
    text = file.read().lower().replace('\n', ' ')

    nlp = spacy.load("ru_core_news_sm")
    doc = nlp(text)
    pos_case = []
    for token in doc:
        pos_case.append((token.pos_, token.morph.get("Case")))

    all_n_grams = list(ngrams(pos_case, 2))

    pairsADP_NN = []
    for elem in all_n_grams:
        ((t1, m1), (t2, m2)) = elem
        if t1 == "ADP" and t2 == "NOUN":
            pairsADP_NN.append((t1, t2, m2))

    pairsADP_PROPN = []
    for elem in all_n_grams:
        ((t1, m1), (t2, m2)) = elem
        if t1 == "ADP" and t2 == "PROPN":
            pairsADP_PROPN.append((t1, t2, m2))

    all_pairs = pairsADP_NN + pairsADP_PROPN

    acc = []
    for a in all_pairs:
        ((t1, t2, m2)) = a
        if m2 == ["Acc"]:
            acc.append((t1, t2, tuple(m2)))
    frequency_acc = collections.Counter(acc)
    dct_frequency_acc = dict(frequency_acc)

    dat = []
    for d in all_pairs:
        ((t1, t2, m2)) = d
        if m2 == ["Dat"]:
            dat.append((t1, t2, tuple(m2)))
    frequency_dat = collections.Counter(dat)
    dct_frequency_dat = dict(frequency_dat)

    gen = []
    for g in all_pairs:
        ((t1, t2, m2)) = g
        if m2 == ["Gen"]:
            gen.append((t1, t2, tuple(m2)))
    frequency_gen = collections.Counter(gen)
    dct_frequency_gen = dict(frequency_gen)

    ins = []
    for i in all_pairs:
        ((t1, t2, m2)) = i
        if m2 == ["Ins"]:
            ins.append((t1, t2, tuple(m2)))
    frequency_ins = collections.Counter(ins)
    dct_frequency_ins = dict(frequency_ins)

    loc = []
    for l in all_pairs:
        ((t1, t2, m2)) = l
        if m2 == ["Loc"]:
            loc.append((t1, t2, tuple(m2)))
    frequency_loc = collections.Counter(loc)
    dct_frequency_loc = dict(frequency_loc)

    nom = []
    for n in all_pairs:
        ((t1, t2, m2)) = n
        if m2 == ["Nom"]:
            nom.append((t1, t2, tuple(m2)))
    frequency_nom = collections.Counter(nom)
    dct_frequency_nom = dict(frequency_nom)

    voc = []
    for v in all_pairs:
        ((t1, t2, m2)) = v
        if m2 == ["Voc"]:
            voc.append((t1, t2, tuple(m2)))
    frequency_voc = collections.Counter(voc)
    dct_frequency_voc = dict(frequency_voc)

    print("acc падеж встречается у NOUN и PROPN, идущего после предлога: ", dct_frequency_acc, "\ndat падеж встречается у NOUN и PROPN, идущего после предлога: ", dct_frequency_dat, "\ngen падеж встречается у NOUN и PROPN, идущего после предлога: ", dct_frequency_gen, "\nins падеж встречается у NOUN и PROPN, идущего после предлога: ", dct_frequency_ins, "\nloc падеж встречается у NOUN и PROPN, идущего после предлога: ", dct_frequency_loc, "\nnom падеж встречается у NOUN и PROPN, идущего после предлога: ", dct_frequency_nom, "\nvoc падеж встречается у NOUN и PROPN, идущего после предлога: ", dct_frequency_voc)

"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
