{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4570e5e1-f5f6-46fd-a36d-6559c271b08d",
   "metadata": {},
   "source": [
    "#### –ó–∞–¥–∞—á–∞ 1. \n",
    "\n",
    "–í–æ–∑—å–º–∏—Ç–µ –ª—é–±–æ–π –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–∂–Ω—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–ª—É—á—à–µ –Ω–µ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, –∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ —Å–æ—Ü—Å–µ—Ç–µ–π). –†–∞–∑–¥–µ–ª–∏—Ç–µ –µ–≥–æ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –≤–∞–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –°—Ä–∞–≤–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ. \n",
    "\n",
    "Choose any Russian text (preferably something with smilies). Split the text into sentences and then into tokens using every known tokenizer and sentenizer. Compare the quality of their work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524c2bd-5fe5-4e30-be0e-dd57f36b9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ru_sent_tokenize import ru_sent_tokenize
from nltk.tokenize import word_tokenize
import spacy

text = """–í—Å–µ –ª—é–¥–∏ —Ä–∞–∑–Ω—ã–µ, –∏ –∫–∞–∂–¥—ã–π —Å–∞–º –≤—ã–±–∏—Ä–∞–µ—Ç,–∫–∞–∫ –ø—Ä–æ—è–≤–ª—è—Ç—å —ç–º–æ—Ü–∏–∏. –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –æ—Ç—Å—Ç—Ä–∞–Ω–µ–Ω–Ω–∞—è –ø–æ–º–æ—â—å - —ç—Ç–æ –±–æ–ª—å—à–æ–π —à–∞–≥ –Ω–∞ –ø—É—Ç–∏ –∫ –ø–æ–¥–¥–µ—Ä–∂–∫–µ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞. 
–î—Ä—É–≥–∏–µ –≥–æ—Ç–æ–≤—ã –ø–æ–∂–µ—Ä—Ç–≤–æ–≤–∞—Ç—å —Å–≤–æ–∏–º —Å–Ω–æ–º/ –∂–∏–∑–Ω—å—é/ –∫–æ–º—Ñ–æ—Ä—Ç–æ–º —Ä–∞–¥–∏ –ø–æ–º–æ—â–∏ –±–ª–∏–∑–∫–æ–º—É :(. –í—Å–µ –ª—é–¥–∏ —Ä–∞–∑–Ω—ã–µ, –∏ –Ω–µ–ª—å–∑—è –æ–±—Å—É–∂–¥–∞—Ç—å –∫–æ–≥–æ-—Ç–æ –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–∫–∞–∑–∞–Ω–Ω—É—é –ø–æ–º–æ—â—å. 
–¶–µ–Ω–∏—Ç–µ —Ç–æ, —á—Ç–æ –∏–º–µ–µ—Ç–µ –∏ —Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –¥–æ–±—Ä–æ–≥–æ –∏ —Ö–æ—Ä–æ—à–µ–≥–æ –æ—Ç–¥–∞–≤–∞—Ç—å –≤ –º–∏—Ä :), —Ç–æ–≥–¥–∞ –∏ –∫ –≤–∞–º –∂–∏–∑–Ω—å –±—É–¥–µ—Ç –¥–æ–±—Ä–∞ ü§ç"""

# NLTK:
all_sentences = ru_sent_tokenize(text)
for sentence in all_sentences:
    # print(word_tokenize(sentence))

# SpaCy - –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç NLTK —Å–º–∞–π–ª—ã ":(" –∏ ":)" –Ω–µ —Ä–∞—Å–ø–∞–¥–∞—é—Ç—Å—è –Ω–∞ –¥–≤–∞, –∞ –≤—ã–≤–æ–¥—è—Ç—Å—è —Ü–µ–ª—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏:
nlp = spacy.load("ru_core_news_sm")
doc = nlp(text)
for token in doc:
         # print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a5dea-5332-4aeb-a022-688e7ce15e7e",
   "metadata": {},
   "source": [
    "#### –ó–∞–¥–∞—á–∞ 2. \n",
    "\n",
    "–í–æ–∑—å–º–∏—Ç–µ –ª—é–±–æ–π –±–æ–ª—å—à–æ–π (–º–æ–∂–Ω–æ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π) —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –ª—É—á—à–µ —Ü–µ–ª–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ. –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ –ª–µ–º–º–∞–º –ø–æ —Ñ–æ—Ä–º—É–ª–µ *–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º / –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Å–µ—Ö —Å–ª–æ–≤ \\* 100*. \n",
    "\n",
    "Take any large Russian text (a novel or something like that) in a .txt - there are plenty [here](https://royallib.com/genre/proza/). Calculate lexical diversity for the text using the formula *the quantity of unique lemmas / total words count \\* 100*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f9b73-2975-4a74-b32d-6dc8f427aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy

punctuations = ".?!,:;-‚Äì‚Äî()[]{}<>‚Äú‚Äù‚Äò/‚Ä¶*&#~\@^|"
txt_no_punc = ""

with open("starik-i-more.txt","r", encoding="utf-8") as file:
        text = file.read().lower().replace('\n', ' ')
        for char in text:
                if char not in punctuations:
                        txt_no_punc += char

        nlp = spacy.load("ru_core_news_sm")
        doc = nlp(txt_no_punc)
        lemmas = []
        for token in doc:
                lemmas.append(token.lemma_)

        unique_lemmas = list()
        for i in lemmas:
                if i not in unique_lemmas:
                        unique_lemmas.append(i)

        number_of_unique_lemmas = len(unique_lemmas)
        number_of_words = len(txt_no_punc.split())

        lexical_diversity = number_of_unique_lemmas / number_of_words * 100

        print(lexical_diversity)
file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070b5c7-c97f-4c6c-a12d-d322565d791f",
   "metadata": {},
   "source": [
    "#### –ó–∞–¥–∞—á–∞ 3. \n",
    "\n",
    "–í–æ–∑—å–º–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ, –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –µ—Å—Ç—å –≤ UD. –°–æ–∑–¥–∞–π—Ç–µ —Å –ø–æ–º–æ—â—å—é –∏—Ö —Å–∞–π—Ç–∞ —Ñ–∞–π–ª .conllu —Å —Ä–∞–∑–±–æ—Ä–æ–º. –° –ø–æ–º–æ—â—å—é –ø–∏—Ç–æ–Ω–∞ –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –ª–µ–º–º –≤ —ç—Ç–æ–º —Ç–µ–∫—Å—Ç–µ (—Ç.–µ. —Ç–µ–∫—Å—Ç –≤–∏–¥–∞ \"Saat be≈üi be≈ü ge√ße\" –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Å–ª–æ–≤–∞—Ä—å-—Å—á–µ—Ç—á–∏–∫ 'saat': 1, 'be≈ü': 2, 'ge√ße': 1). \n",
    "\n",
    "Choose a large text in any language for which there is a model of UDPipe (there is Arabic). Create a .conllu file with the parsing of this text. With the help of Python count lemma frequency in this text (e.g. a text 'ÿßŸÜÿß ÿßŸÇÿ±ÿ£ ŸÉÿ™ÿßÿ®ÿß. ÿßŸÑŸÉÿ™ÿßÿ® ÿ¥ŸäŸÇ' must result in a dictionary-counter \n",
    "\n",
    "    ÿßŸÜÿß: 1, ŸÇŸéÿ±Ÿéÿ£: 1, ŸÉŸêÿ™Ÿéÿßÿ®: 2  ÿ¥ŸäŸÇ: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be2825-4cb0-444e-9634-fd03413adcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse
import collections

with open("processed.conllu", mode="r", encoding="utf-8") as file:
    lines = file.read()
    sentences = parse(lines)

    punctuations = [".", ",", "?", "!", ":", ";", "_"]
    lemmas = []
    for sentence in sentences:
        for token in sentence:
            if str(token) not in punctuations:
                lemmas.append(token["lemma"])
            else:
                break

    frequency = collections.Counter(lemmas)
    dct_frequency = dict(frequency)
    print(dct_frequency)

file.close()
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d77a9-019c-4541-b7a5-ff9247fec8e3",
   "metadata": {},
   "source": [
    "#### –ó–∞–¥–∞—á–∞ 4. \n",
    "\n",
    "–ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ –¥–ª—è –ª—é–±–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –≤–∞—à–µ–º —Ç–µ–∫—Å—Ç–µ –∏–∑ –∑–∞–¥–∞—á–∏ 3. –û—Ü–µ–Ω–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–±–æ—Ä–∞. \n",
    "\n",
    "[Build](https://arborator.ilpga.fr/q.cgi) a graphic dependency tree for any sentence of your text in task 3. Assess the quality of parsing."
 ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45434802-656e-4cc0-b0d3-f32dcde9320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_tree
import spacy
from spacy import displacy


# –ü–æ—Å—Ç—Ä–æ–π–∫–∞ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –¥–µ—Ä–µ–≤–∞ —Å –ø–æ–º–æ—â—å—é conllu:
with open("processed.conllu", mode="r", encoding="utf-8") as file:
    lines = file.read()
    sentences = parse_tree(lines)
    root = sentences[0]
    print(root.print_tree())

file.close()

# –ü–æ—Å—Ç—Ä–æ–π–∫–∞ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –¥–µ—Ä–µ–≤–∞ —Å –ø–æ–º–æ—â—å—é spacy:
with open("processed.conllu", mode="r", encoding="utf-8") as file:
    lines = file.read()
    nlp = spacy.load("fr_core_news_sm")
    doc = nlp(lines)
    tree = displacy.render(doc, style="dep")
    print(tree)

file.close()
"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
