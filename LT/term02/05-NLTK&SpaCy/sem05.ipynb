{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6eb552c-e90b-478a-8a7d-82554113f6c3",
   "metadata": {},
   "source": [
    "Что нужно установить для семинара:\n",
    "\n",
    "    pip install spacy\n",
    "    pip install spacy_udpipe\n",
    "    pip install nltk\n",
    "    pip install corpy\n",
    "    \n",
    "Второе и четвертое могут у вас не установиться без этого: [C++ Build Tools](https://www.microsoft.com/ru-ru/download/details.aspx?id=48159). Почему? Потому что внутри себя udpipe написан в С++, и когда вы устанавливаете эту библиотеку, ваш компьютер должен ее скомпилировать (потому что С++ - компилируемый ЯП и программы, написанные на нем, умеют запускаться сами по себе, в отличие от скриптов питона, которым нужен питон). Для компиляции и нужны эти самые инструменты. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb8642-d0c7-4463-be7d-dee172a5a8ae",
   "metadata": {},
   "source": [
    "Полезные ссылки:\n",
    "\n",
    "[Модели UDpipe](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131)\n",
    "\n",
    "[Модели spacy](https://spacy.io/usage/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d2699-ce9d-4554-8db0-98280ba02ad3",
   "metadata": {},
   "source": [
    "#### Задача 1.\n",
    "\n",
    "Выберите любой понравившийся художественный текст (в текстовом файле! Вставлять текст в код нельзя). Превратите его в объект Text() nltk. Попробуйте поискать вхождения какого-нибудь слова. \n",
    "\n",
    "Choose any text in the genre of fiction (any language would do). You must take a .txt file, pasting the text into your script is not allowed. Transform this text into nltk.Text() object. Try searching any token in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ede0d-42e2-46e0-b22d-4c88b73fc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk

with open('The_Old_Man_and_the_Sea.txt', 'r', encoding='utf-8') as file:
    read_file = file.read().lower()
    text = nltk.Text(nltk.word_tokenize(read_file))
    match = text.concordance('old')
    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf63d55-7d8c-4f81-8dd1-f8d3a4fc521d",
   "metadata": {},
   "source": [
    "#### Задача 2.\n",
    "\n",
    "Выведите 5 самых частотных 4-грамов в этом тексте.\n",
    "\n",
    "Print 5 most frequent 4-grams in this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9533fa-63d7-45b3-b847-af84275cc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk
from nltk.util import ngrams
import collections

punctuations = ".?!,:;’-–—”()[]{}<>“/…*&#~\@^|"
txt_no_punc = ""

with open('The_Old_Man_and_the_Sea.txt', 'r', encoding='utf-8') as file:
    read_file = file.read().lower().replace('\n', ' ')
    for char in read_file:
        if char not in punctuations:
            txt_no_punc += char

    text = nltk.Text(nltk.word_tokenize(txt_no_punc))

    all_n_grams = list(ngrams(text, 4))

    frequency = collections.Counter(all_n_grams)
    frequency_dict = dict(frequency)
    sorted_dict = {k: v for k, v in sorted(frequency_dict.items(), key=lambda item: item[1], reverse=True)}
    lst_sorted_dict = [(k, v) for (k, v) in sorted_dict.items()]

    print(lst_sorted_dict[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df98a32-bf58-4a06-9c64-e3c10b49362f",
   "metadata": {},
   "source": [
    "#### Задача 3. \n",
    "\n",
    "Установите модель spacy для выбранного вами языка. Возьмите маленький текст на этом языке и сделайте его морфоразбор по частям речи. Как вам этот морфоразбор?\n",
    "\n",
    "Install a spacy model for your chosen language. Take a small text in this language and POS-tag it. Are the POS-tags correct? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964c15e-3cc1-4cad-937c-d096a6f515f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy

text = """Все люди разные, и каждый сам выбирает,как проявлять эмоции. Для некоторых отстраненная помощь - это большой шаг на пути к поддержке собеседника. 
Другие готовы пожертвовать своим сном, жизнью, комфортом ради помощи близкому. Все люди разные, и нельзя обсуждать кого-то за неправильно оказанную помощь. 
Цените то, что имеете и старайтесь как можно больше доброго и хорошего отдавать в мир, тогда и к вам жизнь будет добра."""

nlp = spacy.load("ru_core_news_sm")
doc = nlp(text)
for ent in doc:
    print(ent.text,ent.tag_,ent.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59c797-3ee7-4842-b073-2a3a56b20887",
   "metadata": {},
   "source": [
    "#### Задача 4. \n",
    "\n",
    "Лемматизируйте текст на любом выбранном языке с помощью spacy_udpipe или corpy. Посчитайте частотность лемм и выведите топ-5 самых частотных слов для текста.\n",
    "\n",
    "Lemmatize a text in any chosen language with the help of spacy_udpipe or corpy. Count lemma frequency and print top-5 most frequent words for the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b31b4f4-b90f-4f62-850c-a92937fdce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_udpipe
import collections

lines = """Wikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians. 
Wikipedia is the largest and most-read reference work in history.
It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; Wikipedia was ranked the 5th most popular site in the world.
It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.
Wikipedia was launched by Jimmy Wales and Larry Sanger on January 2001. 
Sanger coined its name as a blend of wiki and encyclopedia. 
Wales was influenced by the "spontaneous order" ideas associated with Friedrich Hayek and the Austrian School of economics after being exposed to these ideas by the libertarian economist Mark Thornton.
Initially available only in English, versions in other languages were quickly developed. 
Its combined editions comprise more than 60 million articles, attracting around 2 billion unique device visits per month and more than 15 million edits per month as of January 2023.
In 2006, Time magazine stated that the policy of allowing anyone to edit had made Wikipedia the “biggest encyclopedia in the world“."""

punctuations = ".?!,:;-–—()[]{}<>“”‘/…*&#~\@^|"
txt_no_punc = ""
text = lines.lower().replace('\n', ' ')
for char in text:
    if char not in punctuations:
        txt_no_punc += char

nlp = spacy_udpipe.load("en")
doc = nlp(txt_no_punc)

all_lemmas = []
for token in doc:
    all_lemmas.append(token.lemma_)

frequency = collections.Counter(all_lemmas)
frequency_dict = dict(frequency)
sorted_dict = {k: v for k, v in sorted(frequency_dict.items(), key=lambda item: item[1], reverse=True)}
lst_sorted_dict = [(k, v) for (k, v) in sorted_dict.items()]

print(lst_sorted_dict[0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
