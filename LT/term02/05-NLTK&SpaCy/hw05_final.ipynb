{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd02759-1cda-484b-92ea-7772c6429596",
   "metadata": {},
   "source": [
    "## Итоговое домашнее задание\n",
    "\n",
    "Для выполнения этого задания выберите два любых достаточно длинных текста (.txt) на русском и на любом другом (для которого есть парсеры) языке; если возьмете текст и его перевод, будет отлично. \n",
    "\n",
    "In order to complete these tasks choose two long texts (.txt files) in Russian and in any other language (assumed that there are parsers for it). If you find a text and its translation, it will be more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5dcc1a-4046-481a-9f77-d22065b69382",
   "metadata": {},
   "source": [
    "#### Задача 1. \n",
    "\n",
    "Просмотрите оба выбранных текста. Удостоверьтесь, что тексты чистые, если же в них есть какой-то мусор: хештеги, затесавшиеся при OCR символы и подобное, почистите с помощью регулярных выражений. \n",
    "\n",
    "Проведите первичный статистический анализ: разбейте тексты на предложения и на токены, посчитайте относительное количество того и другого, сопоставьте. Если ваши тексты параллельные, какой длиннее? В каком тексте средняя длина предложения больше? Почему? В каком тексте выше лексическое разнообразие? \n",
    "\n",
    "Таким образом, вам необходимо узнать следующие вещи:\n",
    "\n",
    "- количество предложений (относительное и абсолютное)\n",
    "- количество токенов (относительное и абсолютное)\n",
    "- средняя длина предложения (среднее количество слов в предложении)\n",
    "- соотношение \"уникальные токены / все токены\"\n",
    "- (опционально) соотношение знаков пунктуации и слов\n",
    "\n",
    "Look through your chosen texts. If the texts are not clean (that is, if there are any hashtags or meaningless symbols left after optical character recognition), clean them with the help of regular expressions.\n",
    "\n",
    "Let's analyze the texts from the point of view of statistics: especially if your texts are parallel (translate one another), it is interesting to know the difference between them. You should find:\n",
    "\n",
    "- the quantity of sentences (relative and absolute)\n",
    "- the quantity of tokens (relative and absolute)\n",
    "- the mean length of a sentence (mean number of words in a sentence)\n",
    "- the proportion \"unique token number / all tokens number\"\n",
    "- (optionally) the proportion of punctuation marks and meaningful words"
   ]
  },
  {
import string
import re
import nltk
import numpy as np
from nltk.tokenize import word_tokenize
import spacy

spec_chars = string.punctuation + '«»—…’'

def remove_roman_nums(text):
    pattern = r"\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\b\.?"
    return re.sub(pattern, '', text, flags=re.IGNORECASE)

def remove_arabic_nums(text):
    return "".join([ch for ch in text if ch not in string.digits])

def remove_symbols(text):
     return " ".join(text.split())

def remove_punc(text):
    return "".join([ch for ch in text if ch not in spec_chars])

def number_punc(text):
    count = 0
    for i in text:
        if i in spec_chars:
            count += 1
    return count

with open('Семья Эглетьер-Труая_Анри.txt', 'r', encoding='utf-8') as file_rus:
    read_file_rus = file_rus.read().lower()
    txt_rus = remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_rus)))
    txt_rus_no_punc = remove_punc(txt_rus)

    txt_rus_sentences = nltk.sent_tokenize(txt_rus)

    # абсолютное кол-во токенов (пунктуация не учитывалась) первого текста:
    txt_rus_words = word_tokenize(txt_rus_no_punc)
    txt_rus_number_of_tokens = len(txt_rus_words)

    # соотношение "уникальные токены (по леммам) / все токены" первого текста:
    nlp_rus = spacy.load("ru_core_news_sm")
    doc_rus = nlp_rus(txt_rus_no_punc)
    lemmas_rus = []
    for token in doc_rus:
        lemmas_rus.append(token.lemma_)
    unique_lemmas_rus = list()
    for i in lemmas_rus:
        if i not in unique_lemmas_rus:
            unique_lemmas_rus.append(i)
    txt_rus_number_of_unique_lemmas = len(unique_lemmas_rus)
    txt_rus_correlation_tokens = txt_rus_number_of_unique_lemmas / txt_rus_number_of_tokens

    # абсолютное кол-во предложений первого текста:
    length_rus_sentences = len(txt_rus_sentences)

    # средняя длина предложения (среднее количество слов в предложении) первого текста:
    sentence_word_length_rus = [len(sent.split()) for sent in txt_rus_sentences]
    mean_sentence_len_rus = np.mean(sentence_word_length_rus)

    # (опционально) соотношение знаков пунктуации и слов первого текста:
    txt_rus_words_length = len(txt_rus_words) # кол-во всех слов
    txt_rus_punc_length = number_punc(txt_rus) # кол-во всех знаков пунктуации
    txt_rus_correlation = txt_rus_words_length / txt_rus_punc_length

with open('Les_Eygletiere-Troyat_Henri.txt', 'r', encoding='utf-8') as file_french:
    read_file_french = file_french.read().lower()
    txt_french = remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_french)))
    txt_french_no_punc = remove_punc(txt_french)

    txt_french_sentences = nltk.sent_tokenize(txt_french)

    # абсолютное кол-во токенов (пунктуация не учитывалась) второго текста:
    txt_french_words = word_tokenize(txt_french_no_punc)
    txt_french_number_of_tokens = len(txt_french_words)

    # соотношение "уникальные токены (по леммам) / все токены" второго текста:
    nlp_french = spacy.load("fr_core_news_sm")
    doc_french = nlp_french(txt_french_no_punc)
    lemmas_french = []
    for token in doc_french:
        lemmas_french.append(token.lemma_)
    unique_lemmas_french = list()
    for i in lemmas_french:
        if i not in unique_lemmas_french:
            unique_lemmas_french.append(i)
    txt_french_number_of_unique_lemmas = len(unique_lemmas_french)
    txt_french_correlation_tokens = txt_french_number_of_unique_lemmas / txt_french_number_of_tokens

    # абсолютное кол-во предложений второго текста:
    length_french_sentences = len(txt_french_sentences)

    # средняя длина предложения (среднее количество слов в предложении) второго текста:
    sentence_word_length_french = [len(sent.split()) for sent in txt_french_sentences]
    mean_sentence_len_french = np.mean(sentence_word_length_french)

    # (опционально) соотношение знаков пунктуации и слов второго текста:
    txt_french_words_length = len(txt_french_words)  # кол-во всех слов
    txt_french_punc_length = number_punc(txt_french)  # кол-во всех знаков пунктуации
    txt_french_correlation = txt_french_words_length / txt_french_punc_length

# относительное кол-во предложений:
res_sent = length_rus_sentences / length_french_sentences
# относительное кол-во токенов:
res_tk = txt_rus_number_of_tokens / txt_french_number_of_tokens

print("абсолютное кол-во токенов (пунктуация не учитывалась) русского текста: ", txt_rus_number_of_tokens,
      "\nабсолютное кол-во токенов (пунктуация не учитывалась) французского текста: ", txt_french_number_of_tokens,
      "\nотносительное кол-во токенов обоих текстов: ", res_tk,
      "\nсоотношение *уникальные токены (по леммам) / все токены* русского текста: ", txt_rus_correlation_tokens,
      "\nсоотношение *уникальные токены (по леммам) / все токены* французского текста: ", txt_french_correlation_tokens,
      "\nабсолютное кол-во предложений русского текста: ", length_rus_sentences,
      "\nабсолютное кол-во предложений французского текста: ", length_french_sentences,
      "\nотносительное кол-во предложений обоих текстов: ", res_sent,
      "\nсредняя длина предложения (среднее количество слов в предложении) русского текста: ", mean_sentence_len_rus,
      "\nсредняя длина предложения (среднее количество слов в предложении) французского текста: ", mean_sentence_len_french,
      "\n(опционально) соотношение знаков пунктуации и слов русского текста: ", txt_rus_correlation,
      "\n(опционально) соотношение знаков пунктуации и слов французского текста: ", txt_french_correlation)
  },
  {
   "cell_type": "markdown",
   "id": "57e9f95e-6091-4c9f-afcb-fd266ad561a3",
   "metadata": {},
   "source": [
    "#### Задача 2. \n",
    "\n",
    "Сделайте морфосинтаксические разборы ваших текстов в формате UD, запишите .conllu-файлы. \n",
    "\n",
    "Parse your texts morphologically in UD format, write .conllu-files."
   ]
  },
{
import re
from spacy_conll import init_parser

pattern = r"\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\b\.?"
def remove_roman_nums(text):
    return re.sub(pattern, '', text, flags=re.IGNORECASE)

def remove_symbols(text):
    return " ".join(text.split())


with open('Семья Эглетьер-Труая_Анри.txt', 'r', encoding='utf-8') as file_first:
    read_file_first = file_first.read().lower()
    txt_first = remove_roman_nums(remove_symbols(read_file_first))

    nlp_first = init_parser("ru_core_news_sm",
                      "spacy",
                      ext_names={"conll_pd": "pandas"},
                      conversion_maps={"deprel": {"nsubj": "subj"}})
    doc_first = nlp_first(txt_first)
    file_first_res = doc_first._.pandas

with open('Les_Eygletiere-Troyat_Henri.txt', 'r+', encoding='utf-8') as file_second:
    read_file_second = file_second.read().lower()
    txt_second = remove_roman_nums(read_file_second)

    nlp_second = init_parser("fr_core_news_sm",
                      "spacy",
                      ext_names={"conll_pd": "pandas"},
                      conversion_maps={"deprel": {"nsubj": "subj"}})
    doc_second = nlp_second(txt_first)
    file_second_res = doc_second._.pandas

with open("task_2.conllu", 'w', encoding='utf-8') as outfile:
    print(file_first_res,  file_second_res, file=outfile)
},
  {
   "cell_type": "markdown",
   "id": "d4f40e82-520e-456a-8498-aea94f239467",
   "metadata": {},
   "source": [
    "#### Задача 3. \n",
    "\n",
    "Посчитайте статистику по частям речи, сопоставьте: можно напечатать две таблички с процентами по частям речи. \n",
    "\n",
    "Count POS statistics for both texts: there should be two tables with POS-tags and their quantity in the text, e.g. NOUN 23%."
   ]
  },
{
 import string
import re
import spacy
import collections
from prettytable import PrettyTable

spec_chars = string.punctuation + '«»—…’'

def remove_roman_nums(text):
    pattern = r"\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\b\.?"
    return re.sub(pattern, '', text, flags=re.IGNORECASE)

def remove_arabic_nums(text):
    return "".join([ch for ch in text if ch not in string.digits])

def remove_symbols(text):
     return " ".join(text.split())

def remove_punc(text):
    return "".join([ch for ch in text if ch not in spec_chars])


with open('Семья Эглетьер-Труая_Анри.txt', 'r', encoding='utf-8') as file_rus:
    read_file_rus = file_rus.read().lower()
    txt_rus = remove_punc(remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_rus))))

    nlp_rus = spacy.load("ru_core_news_sm")
    doc_rus = nlp_rus(txt_rus)

    all_pos_rus = []
    for token in doc_rus:
        all_pos_rus.append(token.pos_)

    frequency_rus = collections.Counter(all_pos_rus)
    frequency_dict_rus = dict(frequency_rus)
    sorted_dict_rus = {k: v for k, v in sorted(frequency_dict_rus.items(), key=lambda item: item[1], reverse=True)}
    all_values_rus = sum(frequency_dict_rus.values())
    lst_sorted_dict_rus = [(k, round(v/all_values_rus*100, 2)) for (k, v) in frequency_dict_rus.items()]

    mytable_rus = PrettyTable()
    mytable_rus.field_names = ["POS", "Russian_txt"]
    mytable_rus.add_rows(lst_sorted_dict_rus)
    mytable_rus.align = "r"

with open('Les_Eygletiere-Troyat_Henri.txt', 'r', encoding='utf-8') as file_french:
    read_file_french = file_french.read().lower()
    txt_french = remove_punc(remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_french))))

    nlp_french = spacy.load("fr_core_news_sm")
    doc_french = nlp_french(txt_french)

    all_pos_french = []
    for token in doc_french:
        all_pos_french.append(token.pos_)

    frequency_french = collections.Counter(all_pos_french)
    frequency_dict_french = dict(frequency_french)
    all_values_french = sum(frequency_dict_rus.values())
    lst_sorted_dict_french = [(k, round(v/all_values_french*100, 2)) for (k, v) in frequency_dict_french.items()]

    mytable_french = PrettyTable()
    mytable_french.field_names = ["POS", "Russian_txt"]
    mytable_french.add_rows(lst_sorted_dict_french)
    mytable_french.align = "r"


print("таблица частей речи для русского текста: ", "\n", mytable_rus,
      "\nтаблица частей речи для французского текста: ", "\n", mytable_french)
},
  {
   "cell_type": "markdown",
   "id": "739bdf0e-92f0-4325-8495-1b690e29b52c",
   "metadata": {},
   "source": [
    "#### Задача 4. \n",
    "\n",
    "Посчитайте, какое соотношение токенов по частям речи имеет совпадающие со словоформой леммы (т.е., в скольких случаях токены с частью речи VERB, например, имели словарную форму: и сам токен, и лемма одинаковые). Что вы можете сказать о выбранных вами языках на основании этих данных? Ожидаются две таблички с процентами несовпадающих по лемме и токену слов для каждой части речи. \n",
    "\n",
    "Find out the proportions of tokens whose lemmas differ from their form by POS (e.g., if 30 Nouns out of 100 have different lemmas: 'стола' vs 'стол', then you should print NOUN 30%). What can you say about your languages based on this data? "
   ]
  },
{
import string
import re
import spacy
import collections
from prettytable import PrettyTable
spec_chars = string.punctuation + '«»—…’'

def remove_roman_nums(text):
    pattern = r"\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\b\.?"
    return re.sub(pattern, '', text, flags=re.IGNORECASE)

def remove_arabic_nums(text):
    return "".join([ch for ch in text if ch not in string.digits])

def remove_symbols(text):
     return " ".join(text.split())

def remove_punc(text):
    return "".join([ch for ch in text if ch not in spec_chars])


with open('Семья Эглетьер-Труая_Анри.txt', 'r', encoding='utf-8') as file_rus:
    read_file_rus = file_rus.read().lower()
    txt_rus = remove_punc(remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_rus))))

    nlp_rus = spacy.load("ru_core_news_sm")
    doc_rus = nlp_rus(txt_rus)

    pairs_rus = []
    for ent in doc_rus:
        pairs_rus.append([ent.pos_, ent.text, ent.lemma_])

    pairsTL_rus = []
    notpairsTL_rus = []
    for elem in pairs_rus:
        [p1, w1, t1] = elem
        if w1 == t1: # токен совпадает с леммой
            pairsTL_rus.append([p1, w1, t1])
        else: # токен НЕ совпадает с леммой
            notpairsTL_rus.append([p1, w1, t1])

    dct_part_of_speech_rus = collections.defaultdict(int)
    for elem in notpairsTL_rus:
        dct_part_of_speech_rus[elem[0]] += 1

    all_values_rus = sum(dct_part_of_speech_rus.values())
    lst_sorted_dict_rus = [(k, round(v/all_values_rus*100, 2)) for (k, v) in dct_part_of_speech_rus.items()]

    mytable_rus = PrettyTable()
    mytable_rus.field_names = ["POS", "Russian_txt"]
    mytable_rus.add_rows(lst_sorted_dict_rus)
    mytable_rus.align = "r"

with open('Les_Eygletiere-Troyat_Henri.txt', 'r', encoding='utf-8') as file_french:
    read_file_french = file_french.read().lower()
    txt_french = remove_punc(remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_french))))

    nlp_french = spacy.load("fr_core_news_sm")
    doc_french = nlp_french(txt_french)

    pairs_french = []
    for ent in doc_french:
        pairs_french.append([ent.pos_, ent.text, ent.lemma_])

    pairsTL_french = []
    notpairsTL_french = []
    for elem in pairs_french:
        [p1, w1, t1] = elem
        if w1 == t1:  # токен совпадает с леммой
            pairsTL_french.append([p1, w1, t1])
        else:  # токен НЕ совпадает с леммой
            notpairsTL_french.append([p1, w1, t1])

    dct_part_of_speech_french = collections.defaultdict(int)
    for elem in notpairsTL_french:
        dct_part_of_speech_french[elem[0]] += 1

    all_values_french = sum(dct_part_of_speech_french.values())
    lst_sorted_dict_french = [(k, round(v/all_values_french*100, 2)) for (k, v) in dct_part_of_speech_french.items()]

    mytable_french = PrettyTable()
    mytable_french.field_names = ["POS", "French_txt"]
    mytable_french.add_rows(lst_sorted_dict_french)
    mytable_french.align = "r"


print("таблица частей речи для русского текста: ", "\n", mytable_rus,
      "\nтаблица частей речи для французского текста: ", "\n", mytable_french)
},
  {
   "cell_type": "markdown",
   "id": "659adbbd-f0d9-419d-8b5f-9c15096917cd",
   "metadata": {},
   "source": [
    "#### Задача 5. \n",
    "\n",
    "Посчитайте медианную длину предложения для ваших текстов (медиана - это если взять все длины всех ваших предложений, упорядочить их от маленького к большому и выбрать то число, которое оказалось посередине, а если чисел четное количество, то взять среднее арифметическое двух чисел посередине. Например, если у вас пять предложений длинами 1, 2, 6, 7, 8, то медиана - 6, а если шесть предложений длинами 1, 1, 7, 9, 10, 11, то медиана - (7 + 9) / 2 = 8). Возьмите любые два предложения (одно русское и второе на другом языке) и постройте для них деревья зависимостей. Изучите связи зависимостей (deprel) и вершины: согласны ли вы с разбором?\n",
    "\n",
    "Count median length of a sentence for your text (median is a number which is in the middle of all your sorted sentence length, e.g. if you have five sentences with the lengths 1, 2, 6, 7, 8, then your median is 6, and if there is an even number: 1, 1, 7, 9, 10, 11, then your median is calculated as (7 + 9) / 2 = 8). \n",
    "\n",
    "Take any two sentences with median lengths and build dependency trees (with displacy or any service for .conllu files). Look at dependency relations and their heads: do you agree with this parsing?"
   ]
  },
{
import string
import re
import nltk
import statistics
import spacy
from spacy import displacy

spec_chars = string.punctuation + '«»—…’'

def remove_roman_nums(text):
    pattern = r"\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\b\.?"
    return re.sub(pattern, '', text, flags=re.IGNORECASE)

def remove_arabic_nums(text):
    return "".join([ch for ch in text if ch not in string.digits])

def remove_symbols(text):
     return " ".join(text.split())

def remove_punc(text):
    return "".join([ch for ch in text if ch not in spec_chars])


with open('Семья Эглетьер-Труая_Анри.txt', 'r', encoding='utf-8') as file_rus:
    read_file_rus = file_rus.read().lower()
    txt_rus = remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_rus)))
    txt_rus_no_punc = remove_punc(txt_rus)

    # поиск медианной длины предложения:
    txt_rus_sentences = nltk.sent_tokenize(txt_rus)
    sentence_word_length_rus = [len(sent.split()) for sent in txt_rus_sentences]
    srtlst_rus = sorted(sentence_word_length_rus, reverse=False)
    meadiana_rus = statistics.median(srtlst_rus)

    # построение дерева зависимостей для предложения:
    nlp_rus = spacy.load("ru_core_news_sm")
    doc_rus = nlp_rus(txt_rus_sentences[2])
    tree_rus = displacy.render(doc_rus, style="dep")

with open('Les_Eygletiere-Troyat_Henri.txt', 'r', encoding='utf-8') as file_french:
    read_file_french = file_french.read().lower()
    txt_french = remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_french)))
    txt_french_no_punc = remove_punc(txt_french)

    # поиск медианной длины предложения:
    txt_french_sentences = nltk.sent_tokenize(txt_french)
    sentence_word_length_french = [len(sent.split()) for sent in txt_french_sentences]
    srtlst_french = sorted(sentence_word_length_french, reverse=False)
    meadiana_french = statistics.median(srtlst_french)

    # построение дерева зависимостей для предложения:
    nlp_french = spacy.load("fr_core_news_sm")
    doc_french = nlp_french(txt_french_sentences[2])
    tree_french = displacy.render(doc_french, style="dep")


print("медианная длина русского текста: ", meadiana_rus,
      "\nмедианная длина французского текста: ", meadiana_french,
      "\nдерево зависимостей для русского предложения: ", tree_rus,
      "\nдерево зависимостей для французского предложения: ", tree_french)
},
  {
   "cell_type": "markdown",
   "id": "53c8366f-37c6-489c-b405-7ad5d2a9a121",
   "metadata": {},
   "source": [
    "#### Задача 6. \n",
    "\n",
    "Посчитайте частотные списки токенов для каждой категории связей зависимостей (т.е., нужно выделить все токены в тексте, которые получали, например, ярлык amod, и посчитать их частоты). Выведите по первые три самых частотных токена для каждой категории (punct можно не выводить). \n",
    "\n",
    "Count frequency lists of tokens for each dependency relation category (e.g., for amod relation you should get all tokens which are labelled amod and count their frequencies). Print three first most frequent tokens for each category except for punct. Your result should look like this:\n",
    "\n",
    "    amod: beautiful, ugly, pretty"
   ]
  },
{
import string
import re
import spacy
import collections
spec_chars = string.punctuation + '«»—…’'

def remove_roman_nums(text):
    pattern = r"\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\b\.?"
    return re.sub(pattern, '', text, flags=re.IGNORECASE)

def remove_arabic_nums(text):
    return "".join([ch for ch in text if ch not in string.digits])

def remove_symbols(text):
     return " ".join(text.split())

def remove_punc(text):
    return "".join([ch for ch in text if ch not in spec_chars])

def counterdep(dep):
    dd = collections.defaultdict(int)
    for elem in dep:
        dd[elem[1]] += 1
    return sorted(dd, key=dd.get, reverse=True)[:3]

def depfilter(dep_token):
    amod = counterdep(list(filter(lambda t: 'amod' in t, dep_token)))
    nmod = counterdep(list(filter(lambda t: 'nmod' in t, dep_token)))
    advmod = counterdep(list(filter(lambda t: 'advmod' in t, dep_token)))
    nummod = counterdep(list(filter(lambda t: 'nummod' in t, dep_token)))
    nsubj = counterdep(list(filter(lambda t: 'nsubj' in t, dep_token)))
    csubj = counterdep(list(filter(lambda t: 'csubj' in t, dep_token)))
    obj = counterdep(list(filter(lambda t: 'obj' in t, dep_token)))
    iobj = counterdep(list(filter(lambda t: 'iobj' in t, dep_token)))
    ccomp = counterdep(list(filter(lambda t: 'ccomp' in t, dep_token)))
    xcomp = counterdep(list(filter(lambda t: 'xcomp' in t, dep_token)))
    vocative = counterdep(list(filter(lambda t: 'vocative' in t, dep_token)))
    expl = counterdep(list(filter(lambda t: 'expl' in t, dep_token)))
    advcl = counterdep(list(filter(lambda t: 'advcl' in t, dep_token)))
    aux = counterdep(list(filter(lambda t: 'aux' in t, dep_token)))
    cop = counterdep(list(filter(lambda t: 'cop' in t, dep_token)))
    appos = counterdep(list(filter(lambda t: 'appos' in t, dep_token)))
    det = counterdep(list(filter(lambda t: 'det' in t, dep_token)))
    conj = counterdep(list(filter(lambda t: 'conj' in t, dep_token)))
    case = counterdep(list(filter(lambda t: 'case' in t, dep_token)))
    fixed = counterdep(list(filter(lambda t: 'fixed' in t, dep_token)))
    flat = counterdep(list(filter(lambda t: 'flat' in t, dep_token)))
    compound = counterdep(list(filter(lambda t: 'compound' in t, dep_token)))
    parataxis = counterdep(list(filter(lambda t: 'parataxis' in t, dep_token)))
    root = counterdep(list(filter(lambda t: 'root' in t, dep_token)))
    return ["amod:", amod, "nmod: ", nmod, "advmod: ", advmod, "nummod: ", nummod, "nsubj: ", nsubj, "csubj: ", csubj, "obj: ", obj, "iobj: ", iobj, "ccomp: ", ccomp, "xcomp: ", xcomp, "vocative: ", vocative, "expl: ", expl, "advcl: ", advcl, "aux: ", aux, "cop: ", cop, "appos: ", appos, "det: ", det, "conj: ", conj, "case: ", case, "fixed: ", fixed, "flat: ", flat, "compound: ", compound, "parataxis: ", parataxis, "root: ", root]

with open('Семья Эглетьер-Труая_Анри.txt', 'r', encoding='utf-8') as file_rus:
    read_file_rus = file_rus.read().lower()
    txt_rus = remove_punc(remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_rus))))

    nlp_rus = spacy.load("ru_core_news_sm")
    doc_rus = nlp_rus(txt_rus)
    dep_token_rus = [[token.dep_, token.text] for token in doc_rus]
    res_rus = depfilter(dep_token_rus)

with open('Les_Eygletiere-Troyat_Henri.txt', 'r', encoding='utf-8') as file_french:
    read_file_french = file_french.read().lower()
    txt_french = remove_punc(remove_roman_nums(remove_arabic_nums(remove_symbols(read_file_french))))

    nlp_french = spacy.load("fr_core_news_sm")
    doc_french = nlp_french(txt_french)
    dep_token_french = [[token.dep_, token.text] for token in doc_french]
    res_french = depfilter(dep_token_french)

print("первые три самых частотных токена для каждой категории для русcкого текста: ", "\n", res_rus,
      "\nпервые три самых частотных токена для каждой категории для французского текста: ", "\n", res_french)
}
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
